{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozen Lake Q Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXBYdRqUYZ4QLAN+vmCAnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9characters/RL_repo/blob/master/Frozen_Lake_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yo98xbG0rNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b7b59edd-7652-4941-8852-7b5fb84a4b1a"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlDx8SvFihOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXMCHSo-i0tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "#All the envs are available at gym's website"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHM9hYOFi81p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use the environment to sample state and actions, recieve rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xojv7P44jeeB",
        "colab_type": "text"
      },
      "source": [
        "Now Construct Q-Table and initialize all the Q-values to zero for each state action pair.\n",
        "\n",
        "Number of rows in a table is equal to size of state space in the environment and the number of columns is equal to the number of action space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxeO6O0jrsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKa44mqLja5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a0c17406-891e-467b-b993-dc3574c07fa2"
      },
      "source": [
        "#Build the Q-Table\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpqegKUjkYL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create and Initialize all the parameters needed to implement the Q-Learning algorithm\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100 #For termination, in a episode (receives zero point)\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_rate = 0.99 #gamma symbol\n",
        "\n",
        "exploration_rate = 1 # epsilon value\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 #Change with experiment\n",
        "\n",
        "#The above parameters are to be trained and tuned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WUGzS1elo5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a list to store the rewards in each episode\n",
        "#To see how the game scores change everytime\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    #Resetting to starting state for each episode\n",
        "    state = env.reset()\n",
        "    #To make sure the episode is finished\n",
        "    done = False \n",
        "    rewards_current_episode = 0 #Keep track of reward in each episode\n",
        "\n",
        "    for step in range(max_steps_per_episode): \n",
        "        # Exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0, 1) #This is needed to whether know that our agent will eplore\n",
        "                                                          # or exploit the environment\n",
        "        if exploration_rate_threshold > exploration_rate: # If r>e in explanation, then exploit the environment\n",
        "            action = np.argmax(q_table[state,:])     # And chooses the action with highest Q-value in the Q-table for the current state\n",
        "        else:\n",
        "            action = env.action_space.sample()      #Else explore the environment and sample an action randomly\n",
        "\n",
        "        #After the action is chosen, we then take the action by calling env.step() function --> Execute the action\n",
        "        new_state, reward, done, info = env.step(action) \n",
        "        #env.step() returns a tuple containig 4 items: \n",
        "        #1. new_state 2. reward for the action 3. whether or not action ended our episode \n",
        "        #4. diagnostic information of the environment which might be needed for debugging\n",
        "\n",
        "        # Update Q-table for Q(s,a) as per the formula (weighted sum of old value and learned value)\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "                                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "        state = new_state #Current state is the new state\n",
        "        rewards_current_episode += reward #add the cumulative reward\n",
        "\n",
        "        #check if the episode is over or not\n",
        "        if done == True: \n",
        "          break\n",
        "\n",
        "    # Exploration rate decay\n",
        "    exploration_rate = min_exploration_rate + \\\n",
        "                       (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "                       \n",
        "    #appends the rewards from the current episode               \n",
        "    rewards_all_episodes.append(rewards_current_episode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6BIBxbPrmIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fb89c587-3085-4e5d-9566-dd6f0c4d88f4"
      },
      "source": [
        "# After all episodes are completed, we calculate average reward per 1000 episodes\n",
        "# Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"********Average reward per thousand episodes********\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.12600000000000008\n",
            "2000 :  0.17100000000000012\n",
            "3000 :  0.22200000000000017\n",
            "4000 :  0.2850000000000002\n",
            "5000 :  0.6450000000000005\n",
            "6000 :  0.6560000000000005\n",
            "7000 :  0.6910000000000005\n",
            "8000 :  0.6800000000000005\n",
            "9000 :  0.6910000000000005\n",
            "10000 :  0.6810000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SLIXQX5sHIs",
        "colab_type": "text"
      },
      "source": [
        "Our agent played or 10,000 episodes. At each time-step, the agent received a reward of if it wins and 0 otherwise. We can interpret as at each 1000 episode, agent received a reward at the rate of 16% which changed to 70% win rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYlQp2Rqr7zo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f031f21b-faff-41c8-c0d2-35a2ec56ea2b"
      },
      "source": [
        "# Print updated Q-table\n",
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********Q-table********\n",
            "\n",
            "[[5.56263689e-01 5.06924634e-01 4.89980671e-01 5.09150956e-01]\n",
            " [2.80115432e-01 2.14635920e-01 1.58910064e-01 4.88620639e-01]\n",
            " [4.14903764e-01 2.07351467e-01 2.01000322e-01 2.07526265e-01]\n",
            " [1.09833192e-04 8.49251407e-02 5.62874826e-03 1.65729096e-05]\n",
            " [5.77640667e-01 3.77734042e-01 3.35903483e-01 4.19210209e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.36329130e-01 1.13947299e-01 1.58660790e-01 1.37044676e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.21518386e-01 4.90186305e-01 3.85899138e-01 6.12742299e-01]\n",
            " [5.08694063e-01 6.84070631e-01 4.09422910e-01 3.99965265e-01]\n",
            " [6.21732065e-01 4.00297560e-01 4.29698441e-01 2.90139994e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.06836845e-01 5.87768493e-01 7.69221430e-01 5.25923477e-01]\n",
            " [7.40424845e-01 9.10594917e-01 7.48525249e-01 7.15710202e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx8rZJ_Ys2Xe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7f91bc56-5cd9-472e-84ca-ed4a38bb791a"
      },
      "source": [
        "for episode in range(3):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps_per_episode):        \n",
        "        clear_output(wait=True)\n",
        "        env.render() # render the current state of the environment to the display so \n",
        "                     # that we can visualize the gamegrid and agent gale play\n",
        "        time.sleep(0.3)\n",
        "\n",
        "        action = np.argmax(q_table[state,:])        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # If the episode is completed then we render the environment to see where the agent ended in the\n",
        "        # environment by taking that action\n",
        "        if done:\n",
        "            clear_output(wait=True)\n",
        "            env.render()\n",
        "            if reward == 1:\n",
        "                print(\"****You reached the goal!****\")\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                print(\"****You fell through a hole!****\")\n",
        "                time.sleep(3)\n",
        "            clear_output(wait=True)\n",
        "            break\n",
        "        \n",
        "        state = new_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "****You reached the goal!****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isAAPkodvA7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}